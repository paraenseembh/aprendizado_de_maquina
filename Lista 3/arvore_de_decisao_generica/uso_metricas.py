"""
Exemplo de Uso das M√©tricas de Avalia√ß√£o

Este script demonstra como usar todas as m√©tricas implementadas
para avaliar uma √°rvore de decis√£o.
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from arvore_de_decisao_v1 import DecisionTree
from metricas_v1 import (precision, recall, f1_score, accuracy, specificity,
                     classification_report, print_confusion_matrix, evaluate_model)

# Importar a √°rvore de decis√£o e m√©tricas
# from decision_tree import DecisionTree
# from metrics import (precision, recall, f1_score, accuracy, specificity,
#                      classification_report, print_confusion_matrix, evaluate_model)

# ==============================================================================
# 1. PREPARAR DADOS
# ==============================================================================

# Carregar dataset
iris = load_iris()
X, y = iris.data, iris.target

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Nomes das classes
class_names = ['Setosa', 'Versicolor', 'Virginica']
feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

# ==============================================================================
# 2. TREINAR MODELO
# ==============================================================================

tree = DecisionTree(max_depth=3)
tree.fit(X_train, y_train)

# Fazer predi√ß√µes
y_pred = tree.predict(X_test)

print("Predi√ß√µes realizadas!")
print(f"Total de amostras no teste: {len(y_test)}")

# ==============================================================================
# 3. CALCULAR M√âTRICAS INDIVIDUAIS
# ==============================================================================

print("\n" + "="*70)
print("M√âTRICAS INDIVIDUAIS")
print("="*70)

# Acur√°cia
acc = accuracy(y_test, y_pred)
print(f"\nüìä Acur√°cia: {acc:.4f} ({acc*100:.2f}%)")

# Precis√£o
prec_macro = precision(y_test, y_pred, average='macro')
prec_weighted = precision(y_test, y_pred, average='weighted')
print(f"\nüéØ Precis√£o (Macro): {prec_macro:.4f}")
print(f"üéØ Precis√£o (Weighted): {prec_weighted:.4f}")

# Recall
rec_macro = recall(y_test, y_pred, average='macro')
rec_weighted = recall(y_test, y_pred, average='weighted')
print(f"\nüîç Recall (Macro): {rec_macro:.4f}")
print(f"üîç Recall (Weighted): {rec_weighted:.4f}")

# F1-Score
f1_macro = f1_score(y_test, y_pred, average='macro')
f1_weighted = f1_score(y_test, y_pred, average='weighted')
print(f"\n‚öñÔ∏è  F1-Score (Macro): {f1_macro:.4f}")
print(f"‚öñÔ∏è  F1-Score (Weighted): {f1_weighted:.4f}")

# Especificidade
spec_macro = specificity(y_test, y_pred, average='macro')
spec_weighted = specificity(y_test, y_pred, average='weighted')
print(f"\n‚úÖ Especificidade (Macro): {spec_macro:.4f}")
print(f"‚úÖ Especificidade (Weighted): {spec_weighted:.4f}")

# ==============================================================================
# 4. M√âTRICAS POR CLASSE
# ==============================================================================

print("\n" + "="*70)
print("M√âTRICAS POR CLASSE")
print("="*70)

prec_per_class = precision(y_test, y_pred, average=None)
rec_per_class = recall(y_test, y_pred, average=None)
f1_per_class = f1_score(y_test, y_pred, average=None)
spec_per_class = specificity(y_test, y_pred, average=None)

for cls in prec_per_class.keys():
    cls_name = class_names[cls]
    print(f"\nüìå Classe: {cls_name}")
    print(f"   Precis√£o:      {prec_per_class[cls]:.4f}")
    print(f"   Recall:        {rec_per_class[cls]:.4f}")
    print(f"   F1-Score:      {f1_per_class[cls]:.4f}")
    print(f"   Especificidade: {spec_per_class[cls]:.4f}")

# ==============================================================================
# 5. MATRIZ DE CONFUS√ÉO
# ==============================================================================

print_confusion_matrix(y_test, y_pred, class_names=class_names)

# ==============================================================================
# 6. RELAT√ìRIO COMPLETO
# ==============================================================================

print(classification_report(y_test, y_pred, class_names=class_names))

# ==============================================================================
# 7. AVALIA√á√ÉO COMPLETA (TUDO DE UMA VEZ)
# ==============================================================================

print("\n" + "="*70)
print("AVALIA√á√ÉO COMPLETA DO MODELO")
print("="*70)

results = evaluate_model(tree, X_test, y_test, class_names=class_names)

# Acessar resultados espec√≠ficos
print("\nüì¶ Dicion√°rio de resultados dispon√≠veis:")
for key, value in results.items():
    if not isinstance(value, dict):
        print(f"   {key}: {value:.4f}")

# ==============================================================================
# 8. COMPARAR DIFERENTES CONFIGURA√á√ïES
# ==============================================================================

print("\n" + "="*70)
print("COMPARA√á√ÉO DE DIFERENTES PROFUNDIDADES")
print("="*70)

results_comparison = []

for depth in [2, 3, 5, 10, None]:
    tree_temp = DecisionTree(max_depth=depth)
    tree_temp.fit(X_train, y_train)
    y_pred_temp = tree_temp.predict(X_test)
    
    depth_str = str(depth) if depth else "Ilimitada"
    
    results_comparison.append({
        'depth': depth_str,
        'accuracy': accuracy(y_test, y_pred_temp),
        'precision': precision(y_test, y_pred_temp, average='macro'),
        'recall': recall(y_test, y_pred_temp, average='macro'),
        'f1': f1_score(y_test, y_pred_temp, average='macro'),
        'specificity': specificity(y_test, y_pred_temp, average='macro'),
    })

# Imprimir compara√ß√£o
print(f"\n{'Profundidade':<15} {'Acur√°cia':<12} {'Precis√£o':<12} {'Recall':<12} {'F1-Score':<12} {'Especif.':<12}")
print("-" * 87)

for result in results_comparison:
    print(f"{result['depth']:<15} "
          f"{result['accuracy']:<12.4f} "
          f"{result['precision']:<12.4f} "
          f"{result['recall']:<12.4f} "
          f"{result['f1']:<12.4f} "
          f"{result['specificity']:<12.4f}")

# ==============================================================================
# 9. IDENTIFICAR MELHORES E PIORES CLASSES
# ==============================================================================

print("\n" + "="*70)
print("AN√ÅLISE DE DESEMPENHO POR CLASSE")
print("="*70)

f1_per_class = f1_score(y_test, y_pred, average=None)

best_class = max(f1_per_class.items(), key=lambda x: x[1])
worst_class = min(f1_per_class.items(), key=lambda x: x[1])

print(f"\nüèÜ Melhor classe: {class_names[best_class[0]]} (F1 = {best_class[1]:.4f})")
print(f"‚ö†Ô∏è  Pior classe: {class_names[worst_class[0]]} (F1 = {worst_class[1]:.4f})")

# ==============================================================================
# 10. SALVAR RESULTADOS EM ARQUIVO
# ==============================================================================

def save_results_to_file(y_test, y_pred, class_names, filename='resultados.txt'):
    """Salva todos os resultados em um arquivo de texto."""
    import sys
    from io import StringIO
    
    old_stdout = sys.stdout
    sys.stdout = buffer = StringIO()
    
    print(classification_report(y_test, y_pred, class_names=class_names))
    print_confusion_matrix(y_test, y_pred, class_names=class_names)
    
    sys.stdout = old_stdout
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(buffer.getvalue())
    
    print(f"\nüíæ Resultados salvos em: {filename}")

save_results_to_file(y_test, y_pred, class_names)

# ==============================================================================
# 11. VISUALIZAR M√âTRICAS (OPCIONAL - REQUER MATPLOTLIB)
# ==============================================================================

def plot_metrics_comparison(results_comparison):
    """Plota compara√ß√£o de m√©tricas para diferentes configura√ß√µes."""
    try:
        import matplotlib.pyplot as plt
        
        depths = [r['depth'] for r in results_comparison]
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'specificity']
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = np.arange(len(depths))
        width = 0.15
        
        for i, metric in enumerate(metrics):
            values = [r[metric] for r in results_comparison]
            offset = width * (i - 2)
            ax.bar(x + offset, values, width, label=metric.capitalize())
        
        ax.set_xlabel('Profundidade da √Årvore', fontsize=12)
        ax.set_ylabel('Score', fontsize=12)
        ax.set_title('Compara√ß√£o de M√©tricas por Profundidade', fontsize=14, weight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(depths)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        ax.set_ylim(0, 1.1)
        
        plt.tight_layout()
        plt.savefig('metricas_comparacao.png', dpi=150, bbox_inches='tight')
        print("\nüìä Gr√°fico de compara√ß√£o salvo em: metricas_comparacao.png")
        plt.show()
        
    except ImportError:
        print("\n‚ö†Ô∏è  matplotlib n√£o instalado. Instale com: pip install matplotlib")

# Plotar compara√ß√£o (se matplotlib estiver dispon√≠vel)
plot_metrics_comparison(results_comparison)

# ==============================================================================
# 12. AN√ÅLISE DE ERROS
# ==============================================================================

print("\n" + "="*70)
print("AN√ÅLISE DE ERROS")
print("="*70)

# Identificar amostras classificadas incorretamente
y_test_array = np.array(y_test)
y_pred_array = np.array(y_pred)
errors = y_test_array != y_pred_array
error_indices = np.where(errors)[0]

print(f"\n‚ùå Total de erros: {np.sum(errors)} de {len(y_test)} ({np.sum(errors)/len(y_test)*100:.2f}%)")

if len(error_indices) > 0:
    print("\nüìã Amostras classificadas incorretamente:")
    print(f"{'√çndice':<10} {'Real':<15} {'Predito':<15} {'Features'}")
    print("-" * 70)
    
    for idx in error_indices[:10]:  # Mostrar apenas as 10 primeiras
        real_class = class_names[y_test_array[idx]]
        pred_class = class_names[y_pred_array[idx]]
        features = X_test[idx]
        print(f"{idx:<10} {real_class:<15} {pred_class:<15} {features}")
    
    if len(error_indices) > 10:
        print(f"... e mais {len(error_indices) - 10} erros")

# Matriz de erros por classe
print("\nüìä Distribui√ß√£o de erros por classe:")
for cls in np.unique(y_test_array):
    cls_name = class_names[cls]
    cls_mask = y_test_array == cls
    cls_errors = np.sum(errors & cls_mask)
    cls_total = np.sum(cls_mask)
    error_rate = cls_errors / cls_total if cls_total > 0 else 0
    print(f"   {cls_name:<15}: {cls_errors}/{cls_total} erros ({error_rate*100:.2f}%)")

# ==============================================================================
# 13. ENTENDENDO AS M√âTRICAS
# ==============================================================================

print("\n" + "="*70)
print("GUIA DE INTERPRETA√á√ÉO DAS M√âTRICAS")
print("="*70)

print("""
üéØ PRECIS√ÉO (Precision):
   - O que mede: De todas as predi√ß√µes positivas, quantas estavam corretas?
   - Quando usar: Quando falsos positivos s√£o custosos
   - F√≥rmula: VP / (VP + FP)
   - Exemplo: Em diagn√≥stico m√©dico, evitar dizer que algu√©m est√° doente quando n√£o est√°

üîç RECALL (Sensibilidade):
   - O que mede: De todos os casos positivos reais, quantos foram identificados?
   - Quando usar: Quando falsos negativos s√£o custosos
   - F√≥rmula: VP / (VP + FN)
   - Exemplo: Em detec√ß√£o de fraude, √© cr√≠tico identificar todas as fraudes

‚öñÔ∏è  F1-SCORE:
   - O que mede: M√©dia harm√¥nica entre precis√£o e recall
   - Quando usar: Quando voc√™ quer balancear precis√£o e recall
   - F√≥rmula: 2 √ó (Precis√£o √ó Recall) / (Precis√£o + Recall)
   - Vantagem: Leva em conta tanto falsos positivos quanto falsos negativos

‚úÖ ESPECIFICIDADE:
   - O que mede: De todos os casos negativos reais, quantos foram identificados?
   - Quando usar: Para avaliar a capacidade de identificar casos negativos
   - F√≥rmula: VN / (VN + FP)
   - Exemplo: Identificar corretamente pessoas saud√°veis em um teste m√©dico

üìä ACUR√ÅCIA:
   - O que mede: Propor√ß√£o total de predi√ß√µes corretas
   - Cuidado: Pode ser enganosa em datasets desbalanceados
   - F√≥rmula: (VP + VN) / Total
   - Exemplo: Com 95% de uma classe, um modelo "burro" que sempre prediz 
     essa classe teria 95% de acur√°cia!

üî¢ TIPOS DE M√âDIA:
   - Macro: M√©dia simples (todas as classes t√™m peso igual)
   - Weighted: M√©dia ponderada (classes maiores t√™m mais peso)
   - Micro: Calcula globalmente (soma todos os VP, FP, FN)
""")

# ==============================================================================
# 14. EXEMPLO PR√ÅTICO DE INTERPRETA√á√ÉO
# ==============================================================================

print("\n" + "="*70)
print("EXEMPLO PR√ÅTICO DE INTERPRETA√á√ÉO")
print("="*70)

# Pegar a primeira classe como exemplo
exemplo_classe = 0
cls_name = class_names[exemplo_classe]

prec = precision(y_test, y_pred, average=None)[exemplo_classe]
rec = recall(y_test, y_pred, average=None)[exemplo_classe]
f1 = f1_score(y_test, y_pred, average=None)[exemplo_classe]

print(f"\nüìå An√°lise para classe '{cls_name}':\n")
print(f"   Precis√£o = {prec:.4f}")
print(f"   ‚Üí Interpreta√ß√£o: De todas as vezes que o modelo disse '{cls_name}',")
print(f"     ele acertou {prec*100:.1f}% das vezes.\n")

print(f"   Recall = {rec:.4f}")
print(f"   ‚Üí Interpreta√ß√£o: De todas as amostras que realmente eram '{cls_name}',")
print(f"     o modelo identificou {rec*100:.1f}% delas.\n")

print(f"   F1-Score = {f1:.4f}")
print(f"   ‚Üí Interpreta√ß√£o: Este √© um balanceamento entre precis√£o e recall.")
print(f"     Um F1 de {f1:.4f} indica {'bom' if f1 > 0.8 else 'moderado' if f1 > 0.6 else 'fraco'} desempenho geral.\n")

# ==============================================================================
# 15. DECIS√ïES BASEADAS EM M√âTRICAS
# ==============================================================================

print("\n" + "="*70)
print("GUIA DE DECIS√ÉO BASEADO EM M√âTRICAS")
print("="*70)

print("""
‚ùì QUANDO PRIORIZAR CADA M√âTRICA:

1Ô∏è‚É£  Alta Precis√£o √© cr√≠tica quando:
   - Custo de falso positivo √© alto
   - Recursos s√£o limitados para investigar alertas
   - Exemplo: Sistema de spam (n√£o quero perder emails importantes)

2Ô∏è‚É£  Alto Recall √© cr√≠tico quando:
   - Custo de falso negativo √© alto
   - √â importante n√£o perder nenhum caso positivo
   - Exemplo: Detec√ß√£o de c√¢ncer (n√£o podemos deixar passar nenhum caso)

3Ô∏è‚É£  F1-Score alto √© importante quando:
   - Precis√£o e recall s√£o igualmente importantes
   - Classes est√£o desbalanceadas
   - Exemplo: Classifica√ß√£o de documentos

4Ô∏è‚É£  Alta Especificidade √© cr√≠tica quando:
   - √â importante identificar corretamente os negativos
   - Falsos positivos podem causar alarme desnecess√°rio
   - Exemplo: Testes de triagem m√©dica

5Ô∏è‚É£  Alta Acur√°cia √© suficiente quando:
   - Classes est√£o balanceadas
   - Todos os tipos de erro t√™m custo similar
   - Exemplo: Classifica√ß√£o de d√≠gitos escritos √† m√£o

üí° DICA: Sempre considere o contexto do problema ao escolher a m√©trica!
""")

print("\n" + "="*70)
print("‚úÖ AN√ÅLISE COMPLETA FINALIZADA!")
print("="*70)
print(f"""
üìä Resumo dos Resultados:
   - Acur√°cia Geral: {accuracy(y_test, y_pred):.4f}
   - F1-Score M√©dio: {f1_score(y_test, y_pred, average='macro'):.4f}
   - Melhor Classe: {class_names[best_class[0]]} (F1 = {best_class[1]:.4f})
   - Pior Classe: {class_names[worst_class[0]]} (F1 = {worst_class[1]:.4f})
   - Total de Erros: {np.sum(errors)}/{len(y_test)}

üìÅ Arquivos Gerados:
   - resultados.txt (relat√≥rio completo)
   - metricas_comparacao.png (se matplotlib dispon√≠vel)

üéì Use as m√©tricas apropriadas para seu contexto espec√≠fico!
""")